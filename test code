import requests
import pandas as pd
from typing import Optional, Dict, Any
import logging
from datetime import datetime
import os

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# API Endpoints
API_ENDPOINTS = {
    'permits': 'https://gis-mdc.opendata.arcgis.com/datasets/MDC::building-permits/FeatureServer/0/query',
    'traffic': 'https://gis-mdc.opendata.arcgis.com/datasets/MDC::traffic-incidents/FeatureServer/0/query',
    'crime': 'https://gis-mdc.opendata.arcgis.com/datasets/MDC::crime-incidents/FeatureServer/0/query',
    'public_works': 'https://gis-mdc.opendata.arcgis.com/datasets/MDC::public-works-projects/FeatureServer/0/query'
}

class ArcGISDataFetcher:
    def __init__(self, endpoint: str):
        """
        Initialize the data fetcher with an API endpoint.
        
        Args:
            endpoint (str): The ArcGIS REST API endpoint
        """
        self.endpoint = endpoint
        self.params = {
            'where': '1=1',  # Get all records
            'outFields': '*',  # Get all fields
            'f': 'json',      # Request JSON response
            'resultOffset': 0,
            'resultRecordCount': 2000  # Batch size
        }

    def fetch_data(self) -> Optional[pd.DataFrame]:
        """
        Fetch data from the API with pagination support.
        
        Returns:
            Optional[pd.DataFrame]: DataFrame containing the fetched data or None if failed
        """
        all_features = []
        try:
            while True:
                response = requests.get(self.endpoint, params=self.params)
                response.raise_for_status()
                
                data = response.json()
                features = data.get('features', [])
                
                if not features:
                    break
                    
                all_features.extend(features)
                
                # Update offset for next batch
                self.params['resultOffset'] += self.params['resultRecordCount']
                
                logger.info(f"Fetched {len(all_features)} records so far...")
                
            if all_features:
                # Extract attributes from features
                df = pd.DataFrame([feature['attributes'] for feature in all_features])
                return self._clean_dataframe(df)
            
            return None

        except requests.exceptions.RequestException as e:
            logger.error(f"API request failed: {str(e)}")
            return None
        except Exception as e:
            logger.error(f"Error processing data: {str(e)}")
            return None

    def _clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Clean and normalize the DataFrame.
        
        Args:
            df (pd.DataFrame): Raw DataFrame
            
        Returns:
            pd.DataFrame: Cleaned DataFrame
        """
        # Convert timestamp fields to datetime
        timestamp_columns = df.select_dtypes(include=['int64']).columns
        for col in timestamp_columns:
            if 'date' in col.lower() or 'time' in col.lower():
                df[col] = pd.to_datetime(df[col], unit='ms')
        
        # Remove any completely empty columns
        df = df.dropna(axis=1, how='all')
        
        # Reset index
        df = df.reset_index(drop=True)
        
        return df

def save_to_csv(df: pd.DataFrame, dataset_name: str) -> None:
    """
    Save DataFrame to CSV file with timestamp.
    
    Args:
        df (pd.DataFrame): DataFrame to save
        dataset_name (str): Name of the dataset
    """
    if df is not None and not df.empty:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_dir = 'data'
        os.makedirs(output_dir, exist_ok=True)
        
        filename = f"{output_dir}/{dataset_name}_{timestamp}.csv"
        df.to_csv(filename, index=False)
        logger.info(f"Saved {dataset_name} data to {filename}")
    else:
        logger.warning(f"No data to save for {dataset_name}")

def main():
    """Main execution function"""
    for dataset_name, endpoint in API_ENDPOINTS.items():
        logger.info(f"Fetching {dataset_name} data...")
        
        # Create fetcher instance
        fetcher = ArcGISDataFetcher(endpoint)
        
        # Fetch and process data
        df = fetcher.fetch_data()
        
        # Save to CSV
        save_to_csv(df, dataset_name)

if __name__ == "__main__":
    main() 
